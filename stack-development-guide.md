# Data Engineer Stack Development

Based on: https://dev.to/grayhat/modern-data-engineering-roadmap-2024-thread-with-resources-and-references-1ndn

### Fundamental Programming Languages

- [x] SQL
  - MySQL
  - PostgreSQL
  - T-SQL
- [x] Python
- [ ] Scala (Optional, for MS Azure certification)
- [ ] C++ (Soon)

### Cloud Computing and Distributted Frameworks

- [ ] Core Concepts in Cloud Computing
  - [ ] Infrastructure as a Service (IaaS)
  - [ ] Platform as a Service (PaaS)
  - [ ] Software as a Service (SaaS)
- [ ] Cloud Platforms (Pick 1 atm)
  - [ ] MS Azure
  - Others: Amazon Web Services (AWS) / Google Cloud Platform (GCP)
- [ ] Cloud Services
  - [ ] How to provision resources
  - [ ] How to manage storage
  - [ ] How to deploy applications in cloud envrionment
- [ ] Distributed Computing Frameworks (learn architecture)
  - [ ] Apache Hadoop
  - [ ] Apache Kafka
  - [ ] Apache Spark
    - [ ] PySpark: PySpark is the Python API for Apache Spark, an open source, distributed computing framework and set of libraries for real-time, large-scale data processing. - https://domino.ai/data-science-dictionary/pyspark
  - [ ] Apache Flink

### Data Warehouse and Stream Data Processing

- [ ] Principles of data warehousing

  - [ ] Data modeling
  - [x] Schema design
  - [ ] Optimization

- [ ] Batch data processing

  - [ ] Apache Hive
  - [ ] Amazon Redshift

- [ ] Real-time data processing (with streaming analytics platforms)
  - [ ] Apache Kafka
  - [ ] Apache Flink

### Workflow Orchestration Tools

- [ ] Apache Airflow / Prefect
- [ ] Design workflows
- [ ] Schedule tasks
- [ ] Monitor workflows

### NoSQL databases

- [ ] MongoDB
- [ ] Cassandra
- [ ] Best practices for testing and ensuring data integrity

### Lifelong Upskilling

- Shifting from **ETL to ELT**
- Build **data infrastructure** with cloud platforms
- Create real-time insights and applications with streaming analytics platforms
- Automation (with Airflow) and Democratization (with dbt)
